{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89208933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs (context -> target):\n",
      "['Hello', 'name'] -> my\n",
      "['my', 'is'] -> name\n",
      "['name', 'Jacob'] -> is\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "vocab = { # vocabulary mapping words to indices\n",
    "    \"Hello\": 72,\n",
    "    \"my\": 44,\n",
    "    \"name\": 21,\n",
    "    \"is\": 93,\n",
    "    \"Jacob\": 11,\n",
    "}\n",
    "\n",
    "sentence = [\"Hello\", \"my\", \"name\", \"is\", \"Jacob\"]\n",
    "\n",
    "# Reverse vocab to map indices to words (for debugging)\n",
    "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Convert sentence to indices\n",
    "sentence_idx = [vocab[word] for word in sentence]\n",
    "\n",
    "# Create training pairs (context -> target)\n",
    "half_win_size = 1\n",
    "data = []\n",
    "for i in range(half_win_size, len(sentence_idx) - half_win_size):\n",
    "    context = (\n",
    "        sentence_idx[i - half_win_size : i] \n",
    "        + sentence_idx[i + 1 : i + half_win_size + 1]\n",
    "    )\n",
    "    target = sentence_idx[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "print(\"Training pairs (context -> target):\")\n",
    "for context, target in data:\n",
    "    print(f\"{[idx_to_word[c] for c in context]} -> {idx_to_word[target]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c63cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([72, 21], 44), ([44, 93], 21), ([21, 11], 93)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_cbow_data(tokens, window_size):\n",
    "    \"\"\"\n",
    "    Generate CBOW (context, target) pairs from a list of token indices.\n",
    "    Args:\n",
    "        tokens: List of token indices.\n",
    "        window_size: Number of context words on each side.\n",
    "    Returns:\n",
    "        data: List of (context, target) pairs.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i in range(window_size, len(tokens) - window_size):\n",
    "        context = (\n",
    "            tokens[i - window_size : i] +\n",
    "            tokens[i + 1 : i + window_size + 1]\n",
    "        )\n",
    "        target = tokens[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "# Example Usage:\n",
    "generate_cbow_data(sentence_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a2e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text8\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().strip()  # Read the entire file as a single string\n",
    "\n",
    "# Tokenize into words (text8 is already space-separated)\n",
    "words = text.split()  # List of words (all lowercase, no punctuation)\n",
    "\n",
    "# Count word frequencies (optional, useful for limiting vocab size)\n",
    "word_counts = Counter(words)\n",
    "# word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b99214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
      "Sample words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "def build_vocab_and_tokens(corpus, vocab_size=10000):\n",
    "    \"\"\"\n",
    "    Build vocabulary and convert corpus to tokens.\n",
    "    Args:\n",
    "        corpus: List of words (strings).\n",
    "        vocab_size: Maximum vocabulary size (including <UNK>).\n",
    "    Returns:\n",
    "        tokens: List of token indices.\n",
    "        vocab: Dict mapping word -> index.\n",
    "        idx_to_word: Dict mapping index -> word.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    word_counts = Counter(corpus)\n",
    "    most_common = word_counts.most_common(vocab_size - 1)  # -1 for UNK\n",
    "    vocab = {word: idx + 1 for idx, (word, _) in enumerate(most_common)}\n",
    "    vocab[\"<UNK>\"] = 0\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    tokens = [vocab.get(word, 0) for word in corpus]\n",
    "    return tokens, vocab, idx_to_word\n",
    "\n",
    "# Example usage:\n",
    "tokens, vocab, idx_to_word = build_vocab_and_tokens(words, vocab_size=10000)\n",
    "print(\"Sample tokens:\", tokens[:10])\n",
    "print(\"Sample words:\", [idx_to_word[t] for t in tokens[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1a610f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embs = self.embeddings(inputs)\n",
    "        embs = torch.mean(embs, dim=1)\n",
    "        out = self.linear(embs)\n",
    "        probs = F.log_softmax(out, dim=1)\n",
    "        return probs\n",
    "    \n",
    "    def training_step(self, context, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context: List of word indices (e.g., [72, 44, 93, 11])\n",
    "            target: Target word index (e.g., 21)\n",
    "        Returns:\n",
    "            loss: Tensor with gradients attached\n",
    "        \"\"\"\n",
    "        context = torch.tensor([context], dtype=torch.long)\n",
    "        target = torch.tensor([target], dtype=torch.long).view(-1)\n",
    "        output = self(context)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c88fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example CBOW pairs (as token indices): [(['Hello', 'name'], 'my'), (['my', 'is'], 'name'), (['name', 'Jacob'], 'is')]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "window_size = 1  # context of 1 word on each side\n",
    "demo_words = generate_cbow_data(sentence, window_size) # Do a quick demo with sentence to show it works (in practice this should use the indices not the words)\n",
    "print(\"Example CBOW pairs (as token indices):\", demo_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd323c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_data):\n",
    "        self.data = cbow_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "def train_cbow(\n",
    "    model,\n",
    "    cbow_data,\n",
    "    idx_to_word,\n",
    "    batch_size=128,\n",
    "    num_epochs=5,\n",
    "    lr=0.01,\n",
    "    print_every=1000, # print every n steps\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    dataset = CBOWDataset(cbow_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, (contexts, targets) in enumerate(dataloader):\n",
    "            contexts, targets = contexts.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(contexts)\n",
    "            loss = torch.nn.functional.nll_loss(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % print_every == 0:\n",
    "                # Print a sample prediction for visualization\n",
    "                with torch.no_grad():\n",
    "                    pred_idx = output.argmax(dim=1)[0].item()\n",
    "                    pred_word = idx_to_word.get(pred_idx, str(pred_idx))\n",
    "                    context_words = [idx_to_word.get(c.item(), str(c.item())) for c in contexts[0]]\n",
    "                    target_word = idx_to_word.get(targets[0].item(), str(targets[0].item()))\n",
    "                    print(f\"Epoch {epoch+1} Step {step+1}: Context: {context_words} -> Target: {target_word} | \"\n",
    "                          f\"Predicted: {pred_word} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1} complete. Avg loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db807c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1000: Context: ['after', 'the', 'removal', 'of', 'mother', 'by', 'peter', 'the'] -> Target: his | Predicted: <UNK> | Loss: 8.1623\n",
      "Epoch 1 Step 2000: Context: ['be', 'accomplished', 'when', 'a', 's', 'license', 'is', '<UNK>'] -> Target: driver | Predicted: <UNK> | Loss: 7.6547\n",
      "Epoch 1 Step 3000: Context: ['do', 'not', 'call', 'themselves', 'but', 'are', 'related', 'to'] -> Target: <UNK> | Predicted: <UNK> | Loss: 7.5675\n",
      "Epoch 1 Step 4000: Context: ['a', 'single', '<UNK>', '<UNK>', 'only', 'be', 'parallel', '<UNK>'] -> Target: can | Predicted: <UNK> | Loss: 7.3775\n",
      "Epoch 1 Step 5000: Context: ['influence', 'on', '<UNK>', 'and', 'philosophy', 'of', 'religion', 'on'] -> Target: contemporary | Predicted: <UNK> | Loss: 6.9612\n",
      "Epoch 1 Step 6000: Context: ['of', 'apollo', 'and', 'to', 'apollo', 'the', 'god', 'of'] -> Target: challenge | Predicted: <UNK> | Loss: 7.0594\n",
      "Epoch 1 Step 7000: Context: ['nine', 'six', 'the', 'first', 'of', 'la', '<UNK>', '<UNK>'] -> Target: publication | Predicted: <UNK> | Loss: 6.9671\n",
      "Epoch 1 Step 8000: Context: ['lives', 'sex', '<UNK>', 'became', '<UNK>', 'in', 'the', 'u'] -> Target: normally | Predicted: <UNK> | Loss: 6.5918\n",
      "Epoch 1 Step 9000: Context: ['zero', 'four', 'jack', 'o', 'the', 'one', 'nine', 'four'] -> Target: diamonds | Predicted: <UNK> | Loss: 6.4355\n",
      "Epoch 1 Step 10000: Context: ['is', 'in', '<UNK>', 'contrast', 'most', 'other', 'descriptions', 'from'] -> Target: to | Predicted: <UNK> | Loss: 6.4885\n",
      "Epoch 1 Step 11000: Context: ['four', 'one', 'one', 'nine', 'one', 'hoover', 'herbert', 'c'] -> Target: four | Predicted: <UNK> | Loss: 6.4722\n",
      "Epoch 1 Step 12000: Context: ['one', 'nine', 'marked', 'the', 'of', '<UNK>', 'in', 'american'] -> Target: <UNK> | Predicted: <UNK> | Loss: 6.4399\n",
      "Epoch 1 Step 13000: Context: ['colleges', 'space', 'grant', 'colleges', '<UNK>', 'universities', 'related', 'lists'] -> Target: wikipedia | Predicted: <UNK> | Loss: 6.3245\n",
      "Epoch 1 Step 14000: Context: ['<UNK>', 'bands', 'of', 'high', 'low', 'terrain', 'that', 'extend'] -> Target: and | Predicted: <UNK> | Loss: 6.4069\n",
      "Epoch 1 Step 15000: Context: ['main', 'academic', 'structure', 'these', 'have', 'retained', 'an', 'emphasis'] -> Target: institutions | Predicted: <UNK> | Loss: 6.1274\n",
      "Epoch 1 Step 16000: Context: ['as', 'depicted', 'in', 'the', 'nine', 'th', 'century', 'by'] -> Target: one | Predicted: <UNK> | Loss: 6.7047\n",
      "Epoch 1 Step 17000: Context: ['supports', '<UNK>', 'floppy', 'disk', 'are', 'almost', '<UNK>', 'referred'] -> Target: <UNK> | Predicted: <UNK> | Loss: 6.2729\n",
      "Epoch 1 Step 18000: Context: ['phase', 'velocity', 'or', 'group', 'faster', 'than', 'the', 'vacuum'] -> Target: velocity | Predicted: <UNK> | Loss: 5.9314\n",
      "Epoch 1 Step 19000: Context: ['<UNK>', 'said', '<UNK>', 'he', 'arrived', 'in', '<UNK>', 'not'] -> Target: s | Predicted: <UNK> | Loss: 6.4008\n",
      "Epoch 1 Step 20000: Context: ['drum', 'and', 'bass', 'the', 'between', '<UNK>', 'and', 'drum'] -> Target: difference | Predicted: <UNK> | Loss: 5.8703\n",
      "Epoch 1 Step 21000: Context: ['to', 'change', 'positions', 'and', 'effectively', 'from', 'the', 'back'] -> Target: fight | Predicted: <UNK> | Loss: 6.3170\n",
      "Epoch 1 Step 22000: Context: ['four', 'pp', 'three', 'zero', 'two', 'on', '<UNK>', '<UNK>'] -> Target: three | Predicted: <UNK> | Loss: 5.9627\n",
      "Epoch 1 Step 23000: Context: ['the', 'internet', 'a', 'chart', 'the', 'north', 'american', 'cable'] -> Target: showing | Predicted: <UNK> | Loss: 5.8663\n",
      "Epoch 1 Step 24000: Context: ['in', 'metric', 'terms', 'the', 'metric', 'system', 'was', 'made'] -> Target: original | Predicted: <UNK> | Loss: 5.8176\n",
      "Epoch 1 Step 25000: Context: ['many', '<UNK>', 'symbols', 'of', 'independence', 'from', 'the', 'monarchy'] -> Target: <UNK> | Predicted: <UNK> | Loss: 5.5802\n",
      "Epoch 1 Step 26000: Context: ['seven', 'eight', 'five', '<UNK>', '<UNK>', '<UNK>', 'two', 'zero'] -> Target: <UNK> | Predicted: <UNK> | Loss: 5.9740\n",
      "Epoch 1 Step 27000: Context: ['nine', 'seven', 'for', 'which', 'lived', 'full', 'time', 'in'] -> Target: he | Predicted: <UNK> | Loss: 5.7203\n",
      "Epoch 1 Step 28000: Context: ['<UNK>', 'and', 'the', 'capacity', '<UNK>', 'everything', 'by', 'his'] -> Target: to | Predicted: <UNK> | Loss: 5.6898\n",
      "Epoch 1 Step 29000: Context: ['philosopher', 'and', 'physicist', 'galileo', 'a', 'measure', 'of', '<UNK>'] -> Target: unit | Predicted: <UNK> | Loss: 5.7245\n",
      "Epoch 1 Step 30000: Context: ['as', 'follows', 'item', 'set', 'one', 'e', 'b', 'zero'] -> Target: zero | Predicted: <UNK> | Loss: 6.3068\n",
      "Epoch 1 Step 31000: Context: ['for', 'with', 'the', 're', 'currency', 'of', 'solid', 'gold'] -> Target: invented | Predicted: <UNK> | Loss: 6.0361\n",
      "Epoch 1 Step 32000: Context: ['company', 'and', 'an', 'ability', 'keep', 'within', 'the', '<UNK>'] -> Target: to | Predicted: <UNK> | Loss: 6.2991\n",
      "Epoch 1 Step 33000: Context: ['possible', 'definitions', 'formally', 'we', 'with', 'an', 'algebra', 'd'] -> Target: start | Predicted: <UNK> | Loss: 6.4995\n",
      "Epoch 1 Step 34000: Context: ['and', 'consequently', 'both', 'writers', 'the', 'legend', 'of', 'the'] -> Target: knew | Predicted: <UNK> | Loss: 5.6399\n",
      "Epoch 1 Step 35000: Context: ['regions', 'faced', 'very', 'different', 'prior', 'to', 'the', 'invasion'] -> Target: <UNK> | Predicted: <UNK> | Loss: 6.1365\n",
      "Epoch 1 Step 36000: Context: ['the', 'limit', 'which', 'roughly', 'is', 'a', 'method', 'of'] -> Target: speaking | Predicted: <UNK> | Loss: 6.0885\n",
      "Epoch 1 Step 37000: Context: ['medieval', 'cases', 'the', 'sometimes', 'principles', 'of', '<UNK>', 'and'] -> Target: <UNK> | Predicted: <UNK> | Loss: 5.8723\n",
      "Epoch 1 Step 38000: Context: ['difficulty', 'of', 'the', 'moves', '<UNK>', 'is', 'similar', 'except'] -> Target: <UNK> | Predicted: <UNK> | Loss: 5.8808\n",
      "Epoch 1 Step 39000: Context: ['be', '<UNK>', 'if', 'there', 'a', 'metric', 'd', '<UNK>'] -> Target: is | Predicted: <UNK> | Loss: 6.1319\n",
      "Epoch 1 Step 40000: Context: ['title', 'the', '<UNK>', 'field', '<UNK>', 'so', 'far', 'see'] -> Target: is | Predicted: <UNK> | Loss: 5.8247\n",
      "Epoch 1 Step 41000: Context: ['being', 'required', 'to', 'learn', 'to', 'solve', 'mathematical', 'problems'] -> Target: how | Predicted: <UNK> | Loss: 5.8153\n",
      "Epoch 1 Step 42000: Context: ['iv', 'one', 'nine', 'year', 'passed', 'for', 'two', 'eight'] -> Target: career | Predicted: one | Loss: 6.1517\n",
      "Epoch 1 Step 43000: Context: ['<UNK>', 'ends', 'up', '<UNK>', '<UNK>', 'logic', 'was', 'wrong'] -> Target: his | Predicted: <UNK> | Loss: 5.9860\n",
      "Epoch 1 Step 44000: Context: ['next', 'card', 'face', 'up', 'the', 'center', 'of', 'the'] -> Target: to | Predicted: <UNK> | Loss: 5.6545\n",
      "Epoch 1 Step 45000: Context: ['the', 'sole', 'am', 'radio', 'which', '<UNK>', 'solely', 'in'] -> Target: station | Predicted: <UNK> | Loss: 6.1792\n",
      "Epoch 1 Step 46000: Context: ['wall', 'of', 'the', 'church', 'outside', 'the', '<UNK>', '<UNK>'] -> Target: stands | Predicted: <UNK> | Loss: 5.8695\n",
      "Epoch 1 Step 47000: Context: ['one', 'nine', 'two', 'five', 'nine', 'six', 'four', 't'] -> Target: one | Predicted: one | Loss: 5.7830\n",
      "Epoch 1 Step 48000: Context: ['historians', 'of', 'previous', 'generations', '<UNK>', 'her', '<UNK>', 'in'] -> Target: and | Predicted: <UNK> | Loss: 5.7008\n",
      "Epoch 1 Step 49000: Context: ['sentence', 'was', '<UNK>', 'to', 'years', 'he', '<UNK>', '<UNK>'] -> Target: ten | Predicted: <UNK> | Loss: 5.7037\n",
      "Epoch 1 Step 50000: Context: ['we', 'come', 'from', 'many', 'and', 'cast', 'in', 'our'] -> Target: lands | Predicted: <UNK> | Loss: 5.7806\n",
      "Epoch 1 Step 51000: Context: ['this', 'choice', 'and', 'used', 'define', '<UNK>', 'as', 'a'] -> Target: to | Predicted: <UNK> | Loss: 5.5871\n",
      "Epoch 1 Step 52000: Context: ['<UNK>', 'from', 'the', 'soviet', 'for', 'his', 'book', 'the'] -> Target: union | Predicted: <UNK> | Loss: 5.0998\n",
      "Epoch 1 Step 53000: Context: ['but', 'the', 'only', 'solution', 'the', 'equation', 'ab', 'one'] -> Target: to | Predicted: <UNK> | Loss: 6.1384\n",
      "Epoch 1 Step 54000: Context: ['<UNK>', 'before', 'male', 'a', '<UNK>', 'male', '<UNK>', 'techniques'] -> Target: male | Predicted: <UNK> | Loss: 6.0067\n",
      "Epoch 1 Step 55000: Context: ['right', 'to', 'access', 'the', 'is', 'still', 'strictly', 'restricted'] -> Target: index | Predicted: <UNK> | Loss: 5.8070\n",
      "Epoch 1 Step 56000: Context: ['and', 'third', 'words', 'of', 'titles', 'when', 'in', 'referring'] -> Target: article | Predicted: <UNK> | Loss: 5.7340\n",
      "Epoch 1 Step 57000: Context: ['<UNK>', 'that', 'jordan', 'himself', 'to', '<UNK>', 'by', 'giving'] -> Target: had | Predicted: <UNK> | Loss: 5.5292\n",
      "Epoch 1 Step 58000: Context: ['the', 'one', 'nine', 'eight', '<UNK>', 'cup', 'title', '<UNK>'] -> Target: nine | Predicted: <UNK> | Loss: 6.1704\n",
      "Epoch 1 Step 59000: Context: ['<UNK>', 'liberals', 'also', 'generally', 'any', 'claim', 'of', 'male'] -> Target: reject | Predicted: <UNK> | Loss: 5.7997\n",
      "Epoch 1 Step 60000: Context: ['<UNK>', 'only', 'by', 'adults', 'though', 'the', '<UNK>', '<UNK>'] -> Target: even | Predicted: <UNK> | Loss: 5.4281\n",
      "Epoch 1 Step 61000: Context: ['monarchy', 'march', 'one', 'nine', 'zero', 'with', 'the', 'royal'] -> Target: two | Predicted: <UNK> | Loss: 6.2046\n",
      "Epoch 1 Step 62000: Context: ['knight', '<UNK>', 'are', '<UNK>', 'wealthy', '<UNK>', '<UNK>', 'of'] -> Target: <UNK> | Predicted: <UNK> | Loss: 6.2692\n",
      "Epoch 1 Step 63000: Context: ['ways', 'to', 'measure', 'how', 'the', 'retrieved', 'information', 'matches'] -> Target: well | Predicted: <UNK> | Loss: 5.7972\n",
      "Epoch 1 Step 64000: Context: ['<UNK>', 'in', 'which', 'he', 'alone', 'the', 'idea', 'of'] -> Target: <UNK> | Predicted: <UNK> | Loss: 6.1393\n",
      "Epoch 1 Step 65000: Context: ['by', 'rock', '<UNK>', 'chris', 'collection', 'of', '<UNK>', 's'] -> Target: walter | Predicted: <UNK> | Loss: 5.9174\n",
      "Epoch 1 Step 66000: Context: ['way', 'up', '<UNK>', 'towards', '<UNK>', 'rock', 'while', '<UNK>'] -> Target: the | Predicted: <UNK> | Loss: 6.0678\n",
      "Epoch 1 Step 67000: Context: ['the', 'process', 'of', 'winning', 'called', 'going', '<UNK>', 'which'] -> Target: is | Predicted: <UNK> | Loss: 5.9775\n",
      "Epoch 1 Step 68000: Context: ['<UNK>', '<UNK>', 'in', 'california', '<UNK>', 'for', 'one', 'large'] -> Target: <UNK> | Predicted: <UNK> | Loss: 5.7958\n",
      "Epoch 1 Step 69000: Context: ['may', 'be', '<UNK>', 'by', 'two', '<UNK>', 'vote', 'of'] -> Target: a | Predicted: <UNK> | Loss: 5.9719\n",
      "Epoch 1 Step 70000: Context: ['act', '<UNK>', 'in', 'february', 'zero', 'zero', 'five', '<UNK>'] -> Target: two | Predicted: <UNK> | Loss: 5.5832\n",
      "Epoch 1 Step 71000: Context: ['brother', 'had', 'to', 'face', '<UNK>', 'that', 'he', 'had'] -> Target: and | Predicted: <UNK> | Loss: 6.0234\n",
      "Epoch 1 Step 72000: Context: ['election', 'in', 'most', 'democratic', 'systems', 'there', 'are', 'a'] -> Target: political | Predicted: <UNK> | Loss: 5.8900\n",
      "Epoch 1 Step 73000: Context: ['place', 'in', 'currency', 'the', 'of', 'paper', 'currency', 'were'] -> Target: <UNK> | Predicted: <UNK> | Loss: 5.6032\n",
      "Epoch 1 Step 74000: Context: ['<UNK>', 'ground', '<UNK>', 'than', 'more', 'traditional', '<UNK>', '<UNK>'] -> Target: the | Predicted: <UNK> | Loss: 5.8462\n",
      "Epoch 1 Step 75000: Context: ['incident', 'which', '<UNK>', 'the', 'and', '<UNK>', 'investigation', 'into'] -> Target: world | Predicted: <UNK> | Loss: 5.6456\n",
      "Epoch 1 Step 76000: Context: ['<UNK>', 'american', 'tennis', 'player', 'nine', 'five', 'five', 'jane'] -> Target: one | Predicted: <UNK> | Loss: 5.6133\n",
      "Epoch 1 Step 77000: Context: ['science', 'education', 'or', 'literature', '<UNK>', 'thomas', '<UNK>', '<UNK>'] -> Target: political | Predicted: <UNK> | Loss: 5.8924\n",
      "Epoch 1 Step 78000: Context: ['<UNK>', 'in', '<UNK>', 'by', '<UNK>', '<UNK>', 'the', 'best'] -> Target: jacques | Predicted: <UNK> | Loss: 6.1841\n",
      "Epoch 1 Step 79000: Context: ['spirit', 'of', 'truth', '<UNK>', 'the', 'author', '<UNK>', 'of'] -> Target: life | Predicted: <UNK> | Loss: 5.6176\n",
      "Epoch 1 Step 80000: Context: ['the', 'material', 'in', 'this', 'comes', 'from', 'the', 'cia'] -> Target: article | Predicted: <UNK> | Loss: 5.6829\n",
      "Epoch 1 Step 81000: Context: ['see', '<UNK>', 's', 'e', 'at', '<UNK>', 'com', 'and'] -> Target: <UNK> | Predicted: <UNK> | Loss: 5.8977\n",
      "Epoch 1 Step 82000: Context: ['greek', 'pan', 'became', 'popular', '<UNK>', 'the', 'image', 'of'] -> Target: by | Predicted: <UNK> | Loss: 5.4189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m CBOW(vocab_size, embedding_dim)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_cbow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbow_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_to_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m, in \u001b[0;36mtrain_cbow\u001b[0;34m(model, cbow_data, idx_to_word, batch_size, num_epochs, lr, print_every, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     29\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (contexts, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     31\u001b[0m     contexts, targets \u001b[38;5;241m=\u001b[39m contexts\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/code/mlx_week1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/code/mlx_week1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/code/mlx_week1/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/code/mlx_week1/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mCBOWDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m      9\u001b[0m     context, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(target, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_size=5000\n",
    "num_epochs = 10\n",
    "embedding_dim = 10\n",
    "window_size = 4  # context of 4 words on each side\n",
    "\n",
    "tokens, vocab, idx_to_word = build_vocab_and_tokens(words, vocab_size=vocab_size)\n",
    "cbow_data = generate_cbow_data(tokens, window_size)\n",
    "\n",
    "# Training\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "train_cbow(model, cbow_data, idx_to_word, batch_size=128, num_epochs=5, lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
